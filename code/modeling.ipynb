{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Modeling\n",
    "\n",
    "Notebook to perform text pre-processing and modeling. Preprocessing / transformations will occur simultaneasuly with modelling as we will use GridSearch and other similar techniques to tune the hyperparameters of the model and transformations.\n",
    "\n",
    "> **Data Science Problems**<br> \n",
    "1) *Given the text contained within the title and original post from r/woodworking and r/mtb can we predict which subreddit the post came from with >85% accuracy?*<br> \n",
    "2) Further, using the same model and hyperparameters can we achieve >80% accuracy using the two similar subreddits r/mtb and r/bicycling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Imports & Functions](#Imports-&-Functions)\n",
    "- [Baseline Model & Importing Data](#Baseline-Model-&-Importing-Data)\n",
    "- [Logistic Regression Model](#Logistic-Regression-Model)\n",
    "- [KNN Model](#KNN-Model)\n",
    "- [Naive Bayes Model](#Naive-Bayes-Model)\n",
    "- [Random Forest Model](#Random-Forest-Model)\n",
    "- [VotingClassifier Model](#VotingClassifier-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# General Modeling Imports \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# NLP Imports\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate and display classification metrics, works for bernoulli y\n",
    "def class_metrics(model, X, y):\n",
    "    # Generate predictions\n",
    "    preds = model.predict(X)\n",
    "    # Get confusion matrix and unravel\n",
    "    tn, fp, fn, tp = confusion_matrix(y,preds).ravel()\n",
    "    # Accuracy\n",
    "    print(f'Accuracy: {round((tp+tn)/len(y),3)}')\n",
    "    # Sensitivity\n",
    "    print(f'Sensitivity: {round(tp/(tp+fn),3)}')\n",
    "    # Specificity\n",
    "    print(f'Specificity: {round(tn/(tn+fp),3)}')\n",
    "    # Precision\n",
    "    print(f'Precision: {round(tp/(tp+fp),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzers so that we can stem in our pipelines\n",
    "# Thanks joeln\n",
    "# https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn/36191362\n",
    "\n",
    "# PorterStemmer - CVEC\n",
    "stemmer = PorterStemmer()\n",
    "cvec_analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def porter_cvec_words(doc):\n",
    "    return (stemmer.stem(w) for w in cvec_analyzer(doc))\n",
    "\n",
    "# PorterStemmer - TFIDF\n",
    "tfidf_analyzer = TfidfVectorizer().build_analyzer()\n",
    "\n",
    "def porter_tfidf_words(doc):\n",
    "    return (stemmer.stem(w) for w in tfidf_analyzer(doc))\n",
    "\n",
    "# WordNetLemmatizer - CVEC\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def lemm_cvec_words(doc):\n",
    "    return (lemm.lemmatize(w) for w in cvec_analyzer(doc))\n",
    "\n",
    "# WordNetLemmatizer - TFIDF\n",
    "def lemm_tfidf_words(doc):\n",
    "    return (lemm.lemmatize(w) for w in tfidf_analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model & Importing Data\n",
    "\n",
    "Our data set has equal classes and we will stratify y when performing our train test split. This mean that our baseline accuracy score will be 0.5 since we would get that score if we predicted all posts to be from 1 subbreddit or the other.\n",
    "\n",
    "We won't model this out, but, let's import our data and perform our train test split to get ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is it a bad idea to apply oil based polyuretha...</td>\n",
       "      <td>What are your experiences</td>\n",
       "      <td>1</td>\n",
       "      <td>Is it a bad idea to apply oil based polyuretha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>First project: a needlessly complicated learni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>First project: a needlessly complicated learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Welded this 2x2 steel tube TV console table, u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Welded this 2x2 steel tube TV console table, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Router means: Marble Run! Making toys for my S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Router means: Marble Run! Making toys for my S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>structural feasability</td>\n",
       "      <td>Hey everyone, need some design advice from fol...</td>\n",
       "      <td>1</td>\n",
       "      <td>structural feasability Hey everyone, need some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Is it a bad idea to apply oil based polyuretha...   \n",
       "1  First project: a needlessly complicated learni...   \n",
       "2  Welded this 2x2 steel tube TV console table, u...   \n",
       "3  Router means: Marble Run! Making toys for my S...   \n",
       "4                             structural feasability   \n",
       "\n",
       "                                            selftext  subreddit  \\\n",
       "0                          What are your experiences          1   \n",
       "1                                                NaN          1   \n",
       "2                                                NaN          1   \n",
       "3                                                NaN          1   \n",
       "4  Hey everyone, need some design advice from fol...          1   \n",
       "\n",
       "                                                text  \n",
       "0  Is it a bad idea to apply oil based polyuretha...  \n",
       "1  First project: a needlessly complicated learni...  \n",
       "2  Welded this 2x2 steel tube TV console table, u...  \n",
       "3  Router means: Marble Run! Making toys for my S...  \n",
       "4  structural feasability Hey everyone, need some...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv('../data/subreddit_text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            0\n",
       "selftext     12241\n",
       "subreddit        0\n",
       "text             7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that over half the 'selftext' columns are empty and that 7 'text' columns are empty indicating that the post has neither a title nor text. Let's look at these observations since they shouldn't be empty given all our title columns have text. We will consider droping those 7 observations since we have 20,000 total observations and we can't trust anything we learn from empty posts.\n",
    "\n",
    "Additionally we will focus on the 'text' column to create our X variables as it has the information from both the 'title' and 'selftext' embedded within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>Which is the better table saw? Help</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488</th>\n",
       "      <td>Looking for table saw input. Help</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4492</th>\n",
       "      <td>2x4 45 degree cut off corner scrap as shelf su...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11276</th>\n",
       "      <td>It's been 22 years Bros. Time to start sending...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11333</th>\n",
       "      <td>goodbye Stache, hello MEGA TRS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11336</th>\n",
       "      <td>Goodbye stache! Hello nukeproof</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12453</th>\n",
       "      <td>Used bike w/upgrades. Good idea? Or no? Also f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title selftext  subreddit  \\\n",
       "4434                 Which is the better table saw? Help      NaN          1   \n",
       "4488                   Looking for table saw input. Help      NaN          1   \n",
       "4492   2x4 45 degree cut off corner scrap as shelf su...      NaN          1   \n",
       "11276  It's been 22 years Bros. Time to start sending...      NaN          0   \n",
       "11333                     goodbye Stache, hello MEGA TRS      NaN          0   \n",
       "11336                    Goodbye stache! Hello nukeproof      NaN          0   \n",
       "12453  Used bike w/upgrades. Good idea? Or no? Also f...      NaN          0   \n",
       "\n",
       "      text  \n",
       "4434   NaN  \n",
       "4488   NaN  \n",
       "4492   NaN  \n",
       "11276  NaN  \n",
       "11333  NaN  \n",
       "11336  NaN  \n",
       "12453  NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly something went wrong in when saving over our csv or in cleaning previously. Let's recreate the 'text' column to ensure everything is good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        0\n",
       "selftext     0\n",
       "subreddit    0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill na's with '' so that we can add the string together\n",
    "df['selftext'].fillna('',inplace=True)\n",
    "df['text'] = df['title'] + ' ' + df['selftext']\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our X and y variables\n",
    "X = df['text']\n",
    "y = df['subreddit']\n",
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start our modeling, beginning with a LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Logistic Regression Model we will set up 2 pipelines and GridSearch with a five-fold cross-validation. The difference between our 2 pipelines is that one will use a CountVectorizer and prioritize whichever tokens appear the most often, while the other pipeline will use a TfidfVectorizer which prioritizes tokens that appear often in some documents but not in the whole corpus. Below are the hyperparameters that we will search over to optimize our model.\n",
    "\n",
    "**Vectorizer Hyperparameters**\n",
    "- max_features: Number of features (i.e. tokens) that are outputted from the vectorizer, we will test with 100 and 500 features\n",
    "- stop_words: Whether or not to include sklearn's english stop words, we will test both with and without them\n",
    "- ngram_range: Whether to include only single string tokens or multi-string tokens, we will test with both single string and single and double string\n",
    "- analyzer: Here we will include our custom analyzer from above so that we can test whether using the default 'word' analyzer or a WordNetLemmatizer or PorterStemmer will be best \n",
    "\n",
    "**Regression Hyperparameters**\n",
    "- penalty: We will test with both a ridge and lasso regularization\n",
    "- C: We will attempt standard, strong and weak regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression - CountVectorizer Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cvec__analyzer': <function porter_cvec_words at 0x1a24321200>, 'cvec__max_features': 500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': None, 'lr__C': 1, 'lr__penalty': 'l2'}\n",
      "Accuracy: 0.921\n",
      "Sensitivity: 0.919\n",
      "Specificity: 0.922\n",
      "Precision: 0.922\n"
     ]
    }
   ],
   "source": [
    "# Set up pipleline\n",
    "c_pipe = Pipeline([\n",
    "    ('cvec',CountVectorizer()),\n",
    "    ('lr',LogisticRegression(solver = 'liblinear'))\n",
    "])\n",
    "\n",
    "# Pipe parameters\n",
    "c_pipe_params = {\n",
    "    'cvec__max_features': [100, 500],\n",
    "    'cvec__stop_words': [None,'english'],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'cvec__analyzer': ['word',porter_cvec_words,lemm_cvec_words],\n",
    "    'lr__C': [0.1, 1, 1e9],\n",
    "    'lr__penalty': ['l1','l2']\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "c_gs = GridSearchCV(c_pipe, \n",
    "                    c_pipe_params, \n",
    "                    cv=5,\n",
    "                    n_jobs = 2) \n",
    "\n",
    "# Fit\n",
    "c_gs.fit(X_train,y_train);\n",
    "\n",
    "# Show metrics and best parameters\n",
    "print(c_gs.best_params_)\n",
    "class_metrics(c_gs,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.938\n",
      "Sensitivity: 0.931\n",
      "Specificity: 0.946\n",
      "Precision: 0.945\n"
     ]
    }
   ],
   "source": [
    "class_metrics(c_gs,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem to be very good results, our 4 classification metrics were all ~0.92 which is a strong score. Additionally since in this case we don't have a true 'positive' case we would like to see Sensitivity and Specificity both be similar so we are correctly classifying both subreddits at a similar accuracy. The train and test metrics at most 0.024 apart so it does not seem that there is significant overfitting.\n",
    "\n",
    "This block of code took over 40 minutes to run so going forward we will use some of the results from this GridSearch and assume that certain parameters will be the best for all the upcoming models. While this is not ideal, given computing power and time constraints, we will go forward with the following hyper-parameters set:\n",
    "- Vectorizer analyzer: PorterStemmer\n",
    "- Regression penalty: l2\n",
    "\n",
    "**Best CountVectorizer Logistic Regression**\n",
    "\n",
    "| Hyperparameter | Value |\n",
    "|---|---|\n",
    "| Stemming | Porter |\n",
    "|Max Features|500|\n",
    "|Ngram Range|(1,1)|\n",
    "|Stop Words|None|\n",
    "|C|1|\n",
    "|Penalty|l2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression - TfidfVectorizer Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__C': 1, 'tfidf__max_features': 500, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': None}\n",
      "Accuracy: 0.919\n",
      "Sensitivity: 0.917\n",
      "Specificity: 0.92\n",
      "Precision: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Set up pipleline\n",
    "lr_tf_pipe = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(analyzer=porter_tfidf_words)),\n",
    "    ('lr',LogisticRegression(solver = 'liblinear'))\n",
    "])\n",
    "\n",
    "# Pipe parameters\n",
    "lr_tf_pipe_params = {\n",
    "    'tfidf__max_features': [100, 500],\n",
    "    'tfidf__stop_words': [None,'english'],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'lr__C': [0.1, 1, 1e9]\n",
    "}\n",
    "\n",
    "# tfidf grid search\n",
    "# Instantiate GridSearchCV.\n",
    "lr_tf_gs = GridSearchCV(lr_tf_pipe, \n",
    "                    lr_tf_pipe_params, \n",
    "                    cv=5,\n",
    "                    n_jobs=2) \n",
    "\n",
    "# Fit grid search\n",
    "lr_tf_gs.fit(X_train,y_train);\n",
    "\n",
    "\n",
    "# Show metrics and best parameters\n",
    "print(lr_tf_gs.best_params_)\n",
    "class_metrics(lr_tf_gs,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.935\n",
      "Sensitivity: 0.926\n",
      "Specificity: 0.944\n",
      "Precision: 0.943\n"
     ]
    }
   ],
   "source": [
    "class_metrics(lr_tf_gs,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Logistic Regression using TFIDF Vectorizing gave us very similar, if slightly worse, performace than our previous model using CountVectorizer. Our classification scores all round to 0.92 and are similar whether looking at overall accuracy or accuracy for both classes. Overfitting also does not appear to be an issue with this model. Additionally it was interesting to see that the same hyperparameter values ended up being the best for both models. \n",
    "\n",
    "**Best TfidfVectorizer Logistic Regression**\n",
    "\n",
    "| Hyperparameter | Value |\n",
    "|---|---|\n",
    "| Stemming | Porter |\n",
    "|Max Features|500|\n",
    "|Ngram Range|(1,1)|\n",
    "|Stop Words|None|\n",
    "|C|1|\n",
    "|Penalty|l2|\n",
    "\n",
    "Overall it seems as though our Logistic Regressions are quite good and if we had to chose between the 2 we would use the CountVectorizer transformation. Going forward attempting different models we will simplify our optimizing by assuming that the following vectorizing hyperparameters are set:\n",
    "- Analyser: PorterStemmer\n",
    "- Max Features: 500\n",
    "- Ngram Range: (1,1)\n",
    "- Stop Words: none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X Transformation\n",
    "\n",
    "Since we have decided to keep the same hyperparameters for our transformers we will perform the transformations now so that they do not need to happen as part of the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer \n",
    "# Instantiate\n",
    "cvec = CountVectorizer(analyzer=porter_cvec_words, max_features=500)\n",
    "# Fit\n",
    "cvec.fit(X_train)\n",
    "# Transform\n",
    "C_train = cvec.transform(X_train)\n",
    "C_test = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "C_train = pd.DataFrame(C_train.toarray(),columns=cvec.get_feature_names())\n",
    "C_test = pd.DataFrame(C_test.toarray(),columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer \n",
    "# Instantiate\n",
    "tf = TfidfVectorizer(analyzer=porter_cvec_words, max_features=500)\n",
    "# Fit\n",
    "tf.fit(X_train)\n",
    "# Transform\n",
    "Tf_train = tf.transform(X_train)\n",
    "Tf_test = tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "Tf_train = pd.DataFrame(Tf_train.toarray(),columns=tf.get_feature_names())\n",
    "Tf_test = pd.DataFrame(Tf_test.toarray(),columns=tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our KNN Model we will set up 2 pipelines and GridSearch with a five-fold cross-validation just as we did for our logistic regression. We decided above to utilize standard hyperparameters for the vectorizers, so in this case we will only check our KNN hyperparameters.\n",
    "\n",
    "Note that for our CountVectorizer GridSearch we will need to add a StandardScaler to get our features on the same scale, however, TfidfVectorizer automatically provides scaled outputs from 0 to 1 so it will not be necessary in that pipeline.\n",
    "\n",
    "**KNN Hyperparameter**\n",
    "- k_neighbors: Number of neighbors that have a vote, we will test with 5, 15, 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Model - CountVectorizer Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter: {'n_neighbors': 25}\n",
      "\n",
      "Training Scores\n",
      "Accuracy: 0.848\n",
      "Sensitivity: 0.864\n",
      "Specificity: 0.831\n",
      "Precision: 0.837\n",
      "\n",
      "Test Scores\n",
      "Accuracy: 0.82\n",
      "Sensitivity: 0.847\n",
      "Specificity: 0.794\n",
      "Precision: 0.804\n"
     ]
    }
   ],
   "source": [
    "# Only running a GridSearch Now, no longer need pipeline since we have created\n",
    "# out transformed X_train and X_test\n",
    "\n",
    "# Pipe parameters\n",
    "knn_c_params = {\n",
    "    'n_neighbors': [5,15,25]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "knn_c_gs = GridSearchCV(KNeighborsClassifier(), \n",
    "                    knn_c_params, \n",
    "                    cv=5,\n",
    "                    n_jobs = 2) \n",
    "\n",
    "# Scale Data\n",
    "ss = StandardScaler()\n",
    "C_train_sc = ss.fit_transform(C_train)\n",
    "C_test_sc = ss.transform(C_test)\n",
    "\n",
    "# Fit\n",
    "knn_c_gs.fit(C_train_sc,y_train);\n",
    "\n",
    "# Show metrics and best parameters\n",
    "print(f'Best hyperparameter: {knn_c_gs.best_params_}\\n')\n",
    "print('Training Scores')\n",
    "class_metrics(knn_c_gs,C_train_sc,y_train)\n",
    "print('\\nTest Scores')\n",
    "class_metrics(knn_c_gs,C_test_sc,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial KNN Model performed ]worse than our logistic regression, with test scores around 0.8 - 0.85 vs 0.92 for our logistic regression models. Additionally our KNN model appears to be much better at accurately classifying the woodworking subreddit than the mtb subbreddit as our Sensitivity (i.e. success classifying 1's or woodworking) is 0.05 higher than Specificity. While overfitting does not appear to be an issue here either this model by itself is definitely not our best and may not be worth including in our Voting Classifier either.  Let's see if KNN performs better with a TFIDF Vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Model - TfidfVectorizer Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter: {'n_neighbors': 5}\n",
      "\n",
      "Training Scores\n",
      "Accuracy: 0.835\n",
      "Sensitivity: 0.857\n",
      "Specificity: 0.813\n",
      "Precision: 0.821\n",
      "\n",
      "Test Scores\n",
      "Accuracy: 0.737\n",
      "Sensitivity: 0.787\n",
      "Specificity: 0.686\n",
      "Precision: 0.715\n"
     ]
    }
   ],
   "source": [
    "# GridSearch parameters\n",
    "knn_tf_params = {\n",
    "    'n_neighbors': [5,15,25]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "knn_tf_gs = GridSearchCV(KNeighborsClassifier(), \n",
    "                    knn_tf_params, \n",
    "                    cv=5,\n",
    "                    n_jobs = 2) \n",
    "\n",
    "# Fit\n",
    "knn_tf_gs.fit(Tf_train,y_train);\n",
    "\n",
    "# Show metrics and best parameters\n",
    "print(f'Best hyperparameter: {knn_tf_gs.best_params_}\\n')\n",
    "print('Training Scores')\n",
    "class_metrics(knn_tf_gs,Tf_train,y_train)\n",
    "print('\\nTest Scores')\n",
    "class_metrics(knn_tf_gs,Tf_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFIDF Vectorizer model performed even more poorly with the KNN model, with our test score in the 0.69 - 0.79 range and even more spread between sensitivity and specificity. Additionally, overfitting appears to be an issue with the KNN - Tfidf model. Overall, KNN does not appear to be a good model for this dataset and problem. Interestingly the best number of neighbors was 5 vs 25 for the previous knn model. \n",
    "\n",
    "Let's move on to Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Naive Bayes models we are going to keep the standard hyperparameters and use a MultinomialNB for our Cvec model and GaussianNB for our Tfidf model as those are the types that best fit the X data in the respective cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes Model - CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores\n",
      "Accuracy: 0.92\n",
      "Sensitivity: 0.919\n",
      "Specificity: 0.92\n",
      "Precision: 0.92\n",
      "\n",
      "Test Scores\n",
      "Accuracy: 0.914\n",
      "Sensitivity: 0.925\n",
      "Specificity: 0.903\n",
      "Precision: 0.905\n"
     ]
    }
   ],
   "source": [
    "# Cvec - Multinomial NB\n",
    "# Instantiate\n",
    "mnb = MultinomialNB()\n",
    "# Fit\n",
    "mnb.fit(C_train,y_train)\n",
    "# Metrics\n",
    "print('Training Scores')\n",
    "class_metrics(mnb,C_train,y_train)\n",
    "print('\\nTest Scores')\n",
    "class_metrics(mnb,C_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Cvec - MultinomialNB model is close to as good as our Logistic Regression. There are no signs of overfitting and our scores are in the 0.9 - 0.92 range. One note of caution is that it is about 0.02 better at classifying the woodworking subreddit, whereas, the logistic regression was equally good at both. \n",
    "\n",
    "While not the best model so far, this is definitely worth including in our VotingClassifier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes Model - TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores\n",
      "Accuracy: 0.746\n",
      "Sensitivity: 1.0\n",
      "Specificity: 0.493\n",
      "Precision: 0.663\n",
      "\n",
      "Test Scores\n",
      "Accuracy: 0.737\n",
      "Sensitivity: 0.999\n",
      "Specificity: 0.475\n",
      "Precision: 0.656\n"
     ]
    }
   ],
   "source": [
    "# Tfidf - Gaussian NB\n",
    "# Instantiate\n",
    "gnb = GaussianNB()\n",
    "# Fit\n",
    "gnb.fit(C_train,y_train)\n",
    "# Metrics\n",
    "print('Training Scores')\n",
    "class_metrics(gnb,Tf_train,y_train)\n",
    "print('\\nTest Scores')\n",
    "class_metrics(gnb,Tf_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very interestingly our Gaussian NB - Tfidf model is essentially not missing any woodworking posts, however, it is classifying way too many as woodworking. We can see this with the test preceision score of 0.66 which means that despite getting almost all the woodworking correct we are actually guessing that ~2/3 of the posts are from r/woodworking.\n",
    "\n",
    "This is likely a result of the Tfidf Transformation, but makes it so that this model isn't of much use to us.\n",
    "\n",
    "Next we'll look at a Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model\n",
    "\n",
    "For our Random Forest Models we will once again GridSearch over a couple parameters. We will attempt both a random forest and bagging model as well as optimize over a number of other parameters.\n",
    "\n",
    "**Random Forest Hyperparameters**\n",
    "- n_estimators: Number of trees created, we will attempt 100 and 125\n",
    "- max_depth: How deep the tree is, we will test none (i.e. until all leaves are pure), 10, 25 and 50\n",
    "- max_features: How many features to include in the model, none means that we will use all the features and the model will be a bagging tree while auto uses sqrt(n_features) and is the standard for random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model - CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter: {'max_depth': None, 'max_features': 'auto', 'n_estimators': 125}\n",
      "\n",
      "Training Scores\n",
      "Accuracy: 0.987\n",
      "Sensitivity: 0.995\n",
      "Specificity: 0.978\n",
      "Precision: 0.978\n",
      "\n",
      "Test Scores\n",
      "Accuracy: 0.917\n",
      "Sensitivity: 0.932\n",
      "Specificity: 0.903\n",
      "Precision: 0.906\n"
     ]
    }
   ],
   "source": [
    "# Adapted from GA DSI Lesson 6.03\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 125],\n",
    "    'max_depth': [None, 10, 25, 50],\n",
    "    'max_features': [None, # bagging\n",
    "                     'auto'] # random forest\n",
    "}\n",
    "\n",
    "rf_gs = GridSearchCV(rf, \n",
    "                  param_grid=rf_params,\n",
    "                  cv=5,\n",
    "                  n_jobs=2)\n",
    "\n",
    "rf_gs.fit(C_train,y_train)\n",
    "\n",
    "# Show metrics and best parameters\n",
    "print(f'Best hyperparameter: {rf_gs.best_params_}\\n')\n",
    "print('Training Scores')\n",
    "class_metrics(rf_gs,C_train,y_train)\n",
    "print('\\nTest Scores')\n",
    "class_metrics(rf_gs,C_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest - CountVectorizer model performed quite well with our classification scores between 0.90 and 0.93, this is comparable to the Naive Bayes - CountVectorizer model and is in contention as the 3rd best model behind the 2 Logistic Regression Models. \n",
    "\n",
    "With near perfect training scores it does appear as though there is some overfitting, however, the resulting model is quite good and tree models are prone to overfitting. If this were the final model I might have more concern, but, so far we have other models that have performed better and this is well worth including in our Voting Classifier.\n",
    "\n",
    "Due to the computational requirement of a Random Forest, this cell took very long to run and for our Tfidf random forest we will assume the max_depth and max_features from this GridSearch are also the best. Thus we will remove those form the gridsearch parameters, but add 150 to our n_estimators parameters to see if adding more estimators helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model - TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter: {'n_estimators': 150}\n",
      "\n",
      "Training Scores\n",
      "Accuracy: 0.987\n",
      "Sensitivity: 0.995\n",
      "Specificity: 0.978\n",
      "Precision: 0.978\n",
      "\n",
      "Test Scores\n",
      "Accuracy: 0.918\n",
      "Sensitivity: 0.93\n",
      "Specificity: 0.906\n",
      "Precision: 0.909\n"
     ]
    }
   ],
   "source": [
    "# Adapted from GA DSI Lesson 6.03\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_tf_params = {\n",
    "    'n_estimators': [100, 125, 150],\n",
    "}\n",
    "\n",
    "rf_tf_gs = GridSearchCV(rf, \n",
    "                  param_grid=rf_tf_params,\n",
    "                  cv=5,\n",
    "                  n_jobs=2)\n",
    "\n",
    "rf_tf_gs.fit(Tf_train,y_train)\n",
    "\n",
    "# Show metrics and best parameters\n",
    "print(f'Best hyperparameter: {rf_tf_gs.best_params_}\\n')\n",
    "print('Training Scores')\n",
    "class_metrics(rf_tf_gs,Tf_train,y_train)\n",
    "print('\\nTest Scores')\n",
    "class_metrics(rf_tf_gs,Tf_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest - TfidfVectorizer model performed nearly identically to the previous random forest model. It seems as though different Vectorization techniques do not have much of an effect for the model type given our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingClassifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our voting classifier we need include a couple of separate models and have them vote for each test observation to determine which subreddit a post came from. Let's first look at all of the models we created to decide which one's to include in the voting classifier. (*Note that we can only fit the voting classifier with 1 set of training data so we will organize on vectorizer first and then estimator*)\n",
    "\n",
    "| **Vectorizer**    | **Estimator**             | **Test Accuracy** | **Test Sensitivity** | **Test Specificity** |\n",
    "|-------------------|---------------------------|-------------------|----------------------|----------------------|\n",
    "| *CountVectorizer* | *Logistic Regression*     | 0.921             | 0.919                | 0.922                |\n",
    "| *CountVectorizer* | *KNN*                     | 0.820             | 0.847                | 0.794                |\n",
    "| *CountVectorizer* | *Multinomial Naive Bayes* | 0.914             | 0.925                | 0.903                |\n",
    "| *CountVectorizer* | *Random Forest*           | 0.917             | 0.932                | 0.903                |\n",
    "| *TfidfVectorizer* | *Logistic Regression*     | 0.919             | 0.917                | 0.920                |\n",
    "| *TfidfVectorizer* | *KNN*                     | 0.737             | 0.787                | 0.686                |\n",
    "| *TfidfVectorizer* | *Gaussian Naive Bayes*    | 0.737             | 0.999                | 0.475                |\n",
    "| *TfidfVectorizer* | *Random Forest*           | 0.918             | 0.930                | 0.906                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 8 models we have a clear split between the 5 that performed best and the 3 that did not perform well.\n",
    "\n",
    "**Good Models**\n",
    "- Cvec + Logistic Regression\n",
    "- Cvec + Multinomial Naive Bayes\n",
    "- Cvec + Random Forest\n",
    "- Tfidf + Logistic Regression\n",
    "- Tfidf + Random Forest\n",
    "\n",
    "**Not Great Models**\n",
    "- Cvec + KNN\n",
    "- Tfidf + KNN\n",
    "- Tfidf + Gaussian Naive Bayes\n",
    "\n",
    "All of our 'good' models had score above 0.9 for the 3 metrics we care most about:\n",
    "- Accuracy: Total model accuracy\n",
    "- Sensitivity: Accuracy for r/woodworking\n",
    "- Specificity: Accuracy for r/mtb\n",
    "\n",
    "We saw the best and most stable performance with both of our Logistic Regression models and Random Forest also generally performed well, though it was better at classifying r/woodworking than r/mtb. Since we have 3 good models with CountVectorizer (and even within knn the CountVectorizer model performed better) we will use those 3 for our VotingClassifier, using the hyperparameters we GridSearched for earlier and equal weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores\n",
      "Accuracy: 0.951\n",
      "Sensitivity: 0.946\n",
      "Specificity: 0.957\n",
      "Precision: 0.956\n",
      "\n",
      "Test Scores\n",
      "Accuracy: 0.923\n",
      "Sensitivity: 0.922\n",
      "Specificity: 0.923\n",
      "Precision: 0.923\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Voting Classifier\n",
    "vote = VotingClassifier([\n",
    "            ('lr',LogisticRegression(solver='liblinear')),\n",
    "            ('mnb',MultinomialNB()),\n",
    "            ('rf',RandomForestClassifier(n_estimators=125, random_state=42)) \n",
    "])\n",
    "# Fit \n",
    "vote.fit(C_train,y_train)\n",
    "\n",
    "# metrics\n",
    "print('Training Scores')\n",
    "class_metrics(vote,C_train,y_train)\n",
    "print('\\nTest Scores')\n",
    "class_metrics(vote,C_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Voting Classifier performed very well, scoring a slightly higher than our previous best model (Logistic Regression + CountVectorizer) on the test data. Additionally being very stable and predicting both classes with essentially the same accuracy, ~0.92.\n",
    "\n",
    "While not as interpretable as a stand-alone Logistic Regression, this will be our final model and the one we use to test more similar datasets (i.e. r/mtb and r/bicycling). We will use this model as it scored the best and most consistently of any of the models and intuitively it should be more robust to fitting on different data as the 3 model types within the voting classifier can cover each other's flaws to an extent. \n",
    "\n",
    "As for our first problem: creating a model to classify a text string as coming from 1 of 2 subreddit's with >85% accuracy, these results indicate that we have achieved that goal. Now we will move on to the next notebook (similar_classes.ipynb) ot address our second question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
